[general]
enable_icache_modeling = true

[perf_model/core]
logical_cpus = 1 # number of SMT threads per core
type = interval
core_model = nehalem
frequency = 2.66

[perf_model/core/interval_timer]
dispatch_width = 4
window_size = 128
num_outstanding_loadstores = 10

[perf_model/sync]
reschedule_cost = 1000

[caching_protocol]
type = parametric_dram_directory_msi

[perf_model/branch_predictor]
type = pentium_m
mispredict_penalty=8 # Reflects just the front-end portion (approx) of the penalty for Interval Simulation

[perf_model/tlb]
penalty = 30          # Page walk penalty in cycles

[perf_model/itlb]
size = 128            # Number of I-TLB entries
associativity = 4     # I-TLB associativity

[perf_model/dtlb]
size = 64             # Number of D-TLB entries
associativity = 4     # D-TLB associativity

[perf_model/stlb]
size = 512            # Number of second-level TLB entries
associativity = 4     # S-TLB associativity

[perf_model/cache]
levels = 3

[perf_model/l1_icache]
perfect = false
cache_size = 32
associativity = 4
address_hash = mask
replacement_policy = lru
data_access_time = 4
tags_access_time = 1
perf_model_type = parallel
writethrough = 0
shared_cores = 1

[perf_model/l1_dcache]
perfect = false
cache_size = 32
associativity = 8
address_hash = mask
replacement_policy = lru
data_access_time = 4
tags_access_time = 1
perf_model_type = parallel
writethrough = 0
shared_cores = 1

[perf_model/l2_cache]
perfect = false
cache_size = 256
associativity = 8
address_hash = mask
replacement_policy = lru
data_access_time = 8 # 8.something according to membench, -1 cycle L1 tag access time
# http://www.realworldtech.com/page.cfm?ArticleID=RWT040208182719&p=7
tags_access_time = 3
# Total neighbor L1/L2 access time is around 40/70 cycles (60-70 when it's coming out of L1)
writeback_time = 50 # L3 hit time will be added
perf_model_type = parallel
writethrough = 0
shared_cores = 1

[perf_model/l3_cache]
cache_block_size = 64
address_hash = mask
dvfs_domain = global # L1 and L2 run at core frequency (default), L3 is system frequency
prefetcher = none
writeback_time = 0
perfect = false
cache_size = 8192
associativity = 16
replacement_policy = lru
data_access_time = 30 # 35 cycles total according to membench, +L1+L2 tag times
tags_access_time = 10
perf_model_type = parallel
writethrough = 0
shared_cores = 4

[clock_skew_minimization]
scheme = barrier

[clock_skew_minimization/barrier]
quantum = 100

[dvfs]
transition_latency = 2000 # In ns, "under 2 microseconds" according to http://download.intel.com/design/intarch/papers/323671.pdf (page 8)

[dvfs/simple]
cores_per_socket = 1

[power]
vdd = 1.2 # Volts
technology_node = 45 # nm

[perf_model/dram_directory]
# total_entries = number of entries per directory controller.
total_entries = 1048576
associativity = 16
directory_type = full_map

# Configuration file for different memories -- values aligned to vendor datasheets and JEDEC specs
# Notes:
# - latencies are given as typical "average/random access" estimates in nanoseconds (ns).
#   They reflect realistic access time including common command timings (tRCD + CAS + tRP) as seen in datasheets.
# - per_controller_bandwidth is per 64-bit channel (GB/s): MT/s * 64 / 8.
# - burst_size is expressed in number of beats (BL). With a 64-bit data path, 1 beat = 8 bytes.
# - chips_per_dimm is a typical/common value (varies by density, rank and vendor).

[perf_model/dram]
num_controllers = -1
controllers_interleaving = 4        # default (DDR3/DDR4 = 4, DDR5 = 8)
chips_per_dimm = 8                  # usually 4â€“16, 8 most common in DDR4/DDR5 (McPAT)
dimms_per_controller = 4            # default is 4 for all sniper configs (McPAT)
latency = 80
protocol = ddr3

#include dram/ddr4

# ---- DDR3 (example: DDR3-1333 / PC3-10666) ----
[perf_model/ddr3]
controllers_interleaving = 4
latency = 50                        # ns (example: DDR3-1333 CL9 typical combined tRAS+tRP example in Samsung datasheet). :contentReference[oaicite:3]{index=3}
per_controller_bandwidth = 10.66    # GB/s (1333 MT/s * 64 / 8 = 10.666 GB/s per 64-bit channel). :contentReference[oaicite:4]{index=4}
chips_per_dimm = 8                  # typical x8 single-rank module; dual-rank will effectively have more chips/ranks
burst_size = 8                      # BL=8 (8 beats). DDR3 supports burst-chop (BC=4) as well. :contentReference[oaicite:5]{index=5}

# ---- DDR4 (example: DDR4-2400 / PC4-19200) ----
[perf_model/ddr4]
controllers_interleaving = 4
latency = 45                        # ns (approximate example: DDR4-2400 typical tRCD/tRP/tCL -> example tables show tRCD ~14.16ns, tRP ~14.16ns; combined row miss ~~46ns). :contentReference[oaicite:6]{index=6}
per_controller_bandwidth = 19.2     # GB/s (2400 MT/s * 64 / 8 = 19.2 GB/s per 64-bit channel). :contentReference[oaicite:7]{index=7}
chips_per_dimm = 16                 # common for higher density DIMMs (varies by rank/density)
burst_size = 8                      # BL=8 (DDR4 standard BL=8). :contentReference[oaicite:8]{index=8}

# ---- DDR5 (example: DDR5-4800 base JEDEC speed) ----
[perf_model/ddr5]
controllers_interleaving = 8
latency = 40                        # ns (estimate for DDR5-4800 average access; note DDR5 timing values in cycles (e.g., CL ~40 cycles) -> absolute ns depend on tCK; datasheets show CL values and JEDEC defines speed bins). :contentReference[oaicite:9]{index=9}
per_controller_bandwidth = 38.4     # GB/s (4800 MT/s * 64 / 8 = 38.4 GB/s per 64-bit channel). :contentReference[oaicite:10]{index=10}
chips_per_dimm = 16                 # typical range: 16 for lower-density, 32 for high-density (depends on x4/x8, rank). Adjust to module used.
burst_size = 16                     # BL=16 (DDR5 default), with Burst-Chop options so effective cache-line transfers can be 64 B. :contentReference[oaicite:11]{index=11}